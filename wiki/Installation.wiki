Parsek is a particle in cell (PIC) code based on the implicit moment method. It is the successor of CELESTE ([http://code.google.com/p/celeste/ web page of CELESTE]) but it is fully parallel and written in C++. The code was developed by Stefano Markdis, Enrico Camporeale, David Burgess and myself (Giovanni Lapenta). 

=Installation=
== Before the installation: MPICH2 and required libraries ==

Parsek compiles with mpicxx of '''MPICH2'''. MPICH2 is available at http://www.mcs.anl.gov/research/projects/mpich2/. 

The following libraries are needed: <br>
-'''HDF5 1.6''' used for the output of the simulation data. This library can be downloaded at http://hdf.ncsa.uiuc.edu/HDF5/release/obtain516.html. '''Note that later versions of HDF5, such as the 1.8, are not compatible with Parsek'''. In order to build the HDF5 library, other two libraries must be installed:<br>
-'''szip''' library, availabe at ftp://ftp.hdfgroup.org/lib-external/szip/2.1/src/. <br>
-'''zlib''' library, available at ftp://ftp.hdfgroup.org/lib-external/zlib/1.2/src. <br>

-'''KVF library''', used to parse the input file for Parsek. This library is provided with the parsek source code in the directory /kvf/src/. To build this library, change to the directory /kvf/src and type ''make''.

When installing these libraries, we recommend to install as root, using:
{{{
 ./configure
 make
 sudo make install
}}}

At the end the systems tells the path where the include files and the library has been installed. Make a note of it, it is needed to modify to your needs the make file.

== Install Parsek ==

- prepare a suitable makefile with the specific location of the KVF, HDF5, MPI include files and libraries. A template makefile is provided, but the environment variables INC_KVF, INC_HDF5, INC_MPI, LIB_KVF, LIB_HDF5, LIB_MPI must be set at the beginning of the makefile with the correct path for the KVF, HDF5, MPI include file, and libraries.

- run the makefile, just typing ''make'' in the parsek directory. The makefile prepares the ParsekEM executable.

== Run Parsek ==

- mpirun -np <# processors> ParsekEM <inputfile>

For instance:

mpirun -np 2 ParsekEM inputfile.prit

'''Note that the number of processors after -np should match the number of processors defined in /processtopology/VCtopology.cpp as XLEN times YLEN.'''


==Ubuntu==

For kvf it needs perhaps....

1) To remove from line 40 of  kvfException.h the part: 
KVFException::

2) Install: flexlexer and bison

All instructions for bison and flex are in the makefile.


Put MPI_INT instead of MPI_INTEGER in ComParticles.h

Use makefile.ubuntu



=Running=

==Input file==
The simulation parameters that can be set in the inputfile are:

==== time setup ====
dt = time step <br>
ncycles = number of particles <br>
th = theta, the decentering parameter. This value must be between 0 and 1.
==== simulation box====
Lx = simulation box length - x direction <br>
Ly = simulation box length - y direction <br>
nxc = number of cells - x direction <br>
nyc = number of cells - y direction  <br>
==== particles ====
ns = number of species or groups of particles <br>
TrackParticleID= =true, false --> Assign ID to particles <br>
npcelx = {npcx_1,npcx_2,...,npcx_s} = number of particles per cell - Direction X <br>
npcely = {npcy_1,npcy_2,...,npcy_s} = number of particles per cell - Direction Y <br>
qom = {qom_1,qom_2,...,qom_s} charge to mass ratio for different species <br>
uth  = {uth_1,uth_2,...,uth_s} = thermal velocity for different species - Direction X <br>
vth  = {vth_1,vth_2,...,vth_s} = thermal velocity for different species - Direction Y <br>
wth  = {wth_1,wth_2,...,wth_s} = thermal velocity for different species - Direction Z <br>
u0 = {u0_1,u0_2,...,u0_s} = drift velocity   - Direction X  <br>
v0 = {v0_1,v0_2,...,v0_s} = drift velocity   - Direction Y  <br>
w0 = {w0_1,w0_2,...,w0_s} = drift velocity   - Direction Z  <br>
<hr>
bcPfaceXright = Boundary condition for particles on X right simulation wall. The value can be:
<ul>
<li>0 = exit </li> 
<li>1 = perfect mirror </li>
<li>2 = riemmision </li>
</ul>
bcPfaceXleft =  Boundary condition for particles. Same as above<br>
bcPfaceYright = Boundary condition for particles. Same as above<br>
bcPfaceYleft =  Boundary condition for particles. Same as above<br>
<hr>
Vinj = Injection velocity form the boundary

==== field ====
bcEMfaceXright = Boundary condition for Maxell's solver on X right wall. It value can be <ul>
<li>0 perfect conductor </li>
<li>2 open boundary </li>
<li>3 perfect conductor with no charge on the boundary</li>
<li>4 magnetic mirror </li>
</ul>
bcEMfaceXleft =  Boundary condition for Maxell's solver. As above. <br>
bcEMfaceYright = Boundary condition for Maxell's solver. As above. <br>
bcEMfaceYleft =  Boundary condition for Maxell's solver. As above. <br>
<hr>
B0x = guide magnetic field X direction - used for Harris-like equilibrium<br>
B0y = guide magnetic field Y direction - used for Harris-like equilibrium<br>
B0z = guide magnetic field Z direction - used for Harris-like equilibrium<br>
delta = current sheet thickness - used for Harris-like equilibrium<br>
rhoINIT = {rho_1, rho_} = density amplitude for differnt species<br>
<hr>
bcPHIfaceXright = Boundary Conditions to calculate the potential for the divergence cleaning. set 1 for Dirichilet BC.<br>
bcPHIfaceXleft  = Boundary Conditions to calculate the potential for the divergence cleaning. set 1 for Dirichilet BC.<br>
bcPHIfaceYright = Boundary Conditions to calculate the potential for the divergence cleaning. set 1 for Dirichilet BC.<br>
bcPHIfaceYleft  = Boundary Conditions to calculate the potential for the divergence cleaning. set 1 for Dirichilet BC.<br>

==== smoothing====
Smooth = Smoothing value (5-points stencil)<br>
==== Linear solvers ====
CGtol = stopping criterium tolerance for CG solver for divergence cleaning <br>
GMRES = stopping criterium tolerance for CG solver for divergence cleaning

==== output files ====
SaveDirName = "directory for the proc files";<br>
RestartDirName = "directory for the restart files";
<hr>
ParticlesOutputCycle = each ParticlesOutputCycle, save the Particles data <br>
FieldOutputCycle = each FieldOutputCycle, save the Field and energies data <br>
RestartOutputCycle = each RestartOutputCycle, write the restart file<br>

=== How to include new variables, read from the inputfile ===
To modify the list of variables read from the input file, the files to change are:
# CollectiveIO.h (Mother class)
# Collective.h (subclass, here you define variables and new methods to retrieve the data)
# Collective.cpp (you implement the methods to retrieve the data)

== Parsek Output ==
Parsek output is in the HDF5 format. There are two kind of output files:
# '''proc0.hdf, proc1.hdf,....procN.hdf''' files (as many files as the number of prcessors). with all the information of the simulation. This file can be read only when the simulation is done
# '''restart0.hdf, restart1.hdf, ..., restartN.hdf'''. These files contain all the information about particles  and fields to enable a restart. This file can be opened while parsek is running, and be used to mponitor the simulation



===Read the HDF5 Output with Matlab===
The output is in HDF5 and can be read with matlab using a special file (parsek2D.m for 2D). Once matlab has the data it can be displayed in any usual way. The files are in matlab/parsek. No need to call parsek2D.m first, as it is already called by the other files when needed.

==Parallel Topology==
To change the number of processors and their topology:
# change the call to mpirun (in mpi2D.sh) specifying the right total number of processors
# change XLEN e YLEN in '''VCTopology.cpp''' in the subdirectory processtopology

==Submitting a Parsek job on VIC==
The VIC cluster uses the PBS system to submit jobs. To submit a job on VIC, use:

# '''qsub mpi2D.sh'''

where ''mpi2D.sh'' is a script, that includes the number of processors and time required for the simulation. A typical script file, called ''mpi2D.sh'', can be found in the parsek directory.
You need to modify the directory where your executable(ParsekEM) is, after cd, and the name of the inputfile for Parsek. In addition you need to choose the number of processors after -lnodes, and the time -l walltime.

==Monitoring Parallel Jobs on VIC==

# '''qstat -a''' shows the status of all the jobs you are running.
# '''qstat -n''' shows the status of the jobs you are running. In addition to the basic information, nodes allocated to a job are listed.
# '''showq''' shows all the queu of jobs on VIC cluster. It includes running, in idle, an in hold jobs.
# '''showstart job_ID''' shows an estimated time for the start of the submitted job with job_ID number.

To have a graphical interface of the jobs running on VIC, visit 

http://login.vic.cc.kuleuven.be/ganglia/index.php